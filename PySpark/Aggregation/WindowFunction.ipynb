{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e6e75423-7bec-4b08-96b2-b68caf0ab956",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# PySpark Window Function\n",
    "PySpark window functions are a set of SQL-like operations that allow you to perform calculations across a group of rows that are related to the current row, but without collapsing the rows into a single row. These functions are particularly useful for tasks such as ranking, aggregating over specific partitions, and calculating cumulative or rolling statistics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b4c99b7b-5260-49ab-8fc3-a3553c61677c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Key Features of PySpark Window Functions**\n",
    "\n",
    "1. Operate on a \"Window\" of Rows: Define a subset of data (the \"window\") for each row based on certain criteria like partitioning and ordering.\n",
    "1. Non-collapsing: Unlike groupBy, window functions keep the number of rows unchanged.\n",
    "1. SQL and Functional API: Can be used with both SQL queries and PySpark's DataFrame API.\n",
    "\n",
    "\n",
    "**Common Use Cases**<br>\n",
    "- **Ranking rows**: Assign ranks to rows within a partition.\n",
    "- **Cumulative calculations**: Compute running totals, averages, etc.\n",
    "- **Lag/Lead**: Access previous or next row values.\n",
    "- **Aggregations**: Perform operations like min, max, avg over a specified window."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9efe815b-9787-4602-85c1-f589a0d2eecb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Syntax**<br>\n",
    "Using a window function involves three steps:\n",
    "\n",
    "1. Define the Window:\n",
    "    - Partition: Specifies the grouping of rows.\n",
    "    - Order: Specifies the sorting within each partition.\n",
    "1. Apply the Function: Perform an operation (e.g., sum, rank).\n",
    "1. Use the Result: Add the calculated column to the DataFrame."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f746cbf6-6aff-48b3-9c83-044a1eb440c5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Examples**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bc0aa3ea-b257-418a-9265-9a307c351f6b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize the SparkSession with a specific application name\n",
    "spark = (SparkSession.builder\n",
    "         .appName('PySpark Window Function')\n",
    "         .getOrCreate())\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f58c52c8-150f-45f0-878a-e2674dd769f8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.window import Window"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "92631056-4a2b-4f12-b6d1-520046a9b687",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Ranking**: Rank products within each category based on sales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "63ceb3f6-9e19-4f90-8568-bb2ea5fcffd2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "data = [\n",
    "    (\"James\", \"Sales\", 3000),\n",
    "    (\"Michael\", \"Sales\", 4600),\n",
    "    (\"Robert\", \"Sales\", 4100),\n",
    "    (\"Maria\", \"Finance\", 3000),\n",
    "    (\"Scott\", \"Finance\", 3300),\n",
    "    (\"Jen\", \"Finance\", 3900),\n",
    "    (\"Jeff\", \"Marketing\", 3000),\n",
    "    (\"Kumar\", \"Marketing\", 2000),\n",
    "    (\"Saif\", \"Sales\", 4100)\n",
    "]\n",
    "\n",
    "columns= [\"employee_name\", \"department\", \"salary\"]\n",
    "df = spark.createDataFrame(data = data, schema = columns)\n",
    "\n",
    "df.printSchema()\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "be88549a-c2b7-4ddd-ad64-b8d096c45cb1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Ranking functions\n",
    "\n",
    "PySparkâ€™s Window Ranking functions, such as row_number(), rank(), and dense_rank(), are used to assign unique identifiers or ranks to rows within a specific partition of a dataset. These functions operate over a window, which is a subset of data defined by a partitioning and ordering logic. They are useful for tasks like ordering, ranking, and identifying specific rows based on the specified conditions.\n",
    "\n",
    "**Key Concepts**\n",
    "- **Partition**: Divides the dataset into groups based on one or more columns (e.g., department).\n",
    "- **Ordering**: Determines the sequence of rows within each partition (e.g., by salary).\n",
    "- **Sequential Assignment**: These functions assign numbers to rows in the order defined by the partition and sorting criteria.\n",
    "\n",
    "**Key Benefits**\n",
    "- **Enhanced Data Insights**: Easily analyze and compare rows within groups.\n",
    "- **Versatility**: Useful in real-world scenarios like leaderboard rankings, pagination, and top-N analysis.\n",
    "- **Control Over Ties**: Choose between rank and dense_rank depending on how you want to handle ties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4e86c21e-6e33-49c2-8bc8-8844dfb8888f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "window_spec_ex  = Window.partitionBy(\"department\").orderBy(\"salary\")\n",
    "\n",
    "df.withColumn(\"row_number\",row_number().over(window_spec_ex)) \\\n",
    "    .withColumn(\"rank\",rank().over(window_spec_ex)) \\\n",
    "    .withColumn(\"dense_rank\",dense_rank().over(window_spec_ex)) \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "88b4b2df-8d1d-4f7b-a728-af33c768033c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Top Selling Product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b91e4603-511e-46f3-afd5-f1a282661087",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ../DatasetSourcePath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0c12edcd-3abb-4a59-95ce-1a6653592c28",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "path = sourcePath + '/dataset/sales_data_cleaned.csv'\n",
    "salesdf = spark.read.csv(\n",
    "               path, \n",
    "               header=True, \n",
    "               inferSchema=True).drop('Month')\n",
    "\n",
    "print(salesdf.count())\n",
    "salesdf.printSchema()\n",
    "salesdf.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "82840122-fc1d-4bad-95a7-6e5476d8ce5d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "tot_salesdf = (salesdf\n",
    "               .filter(\"Category != 'Unknown'\")\n",
    "               .groupBy('Category', 'Product')\n",
    "               .agg(\n",
    "                   round(sum(col(\"Sales\") * col(\"Quantity\"))).alias(\"TotalSales\")\n",
    "               ))\n",
    "tot_salesdf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "47b84ce6-ec2f-4288-b874-de2323384bc5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "window_spec = Window.partitionBy(\"Category\").orderBy(col(\"TotalSales\").desc())\n",
    "\n",
    "# Apply the ranking functions\n",
    "ranked_df = (tot_salesdf\n",
    "             .withColumn(\"row_number\", row_number().over(window_spec))\n",
    "             .filter(col(\"row_number\") == 1)\n",
    "             .select('Category', 'Product', 'TotalSales')\n",
    "             .sort(col('TotalSales').desc()))\n",
    "\n",
    "ranked_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "23177d6d-aae6-4a54-8748-aed6fa56db33",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### ntile window function\n",
    "\n",
    "The ```ntile() ```window function in PySpark is used to distribute rows of data into a specified number of buckets or groups, based on the ordering of the rows within a partition. The rows are evenly divided into the given number of buckets, and each row is assigned a bucket number from 1 to n, where n is the number of buckets.\n",
    "\n",
    "**How ntile() works:**\n",
    "- The function takes a single argument, which is the number of buckets (n) to divide the data into.\n",
    "- It returns the bucket number for each row in the ordered set.\n",
    "- The rows are first ordered according to a specified column and then divided into n groups as evenly as possible.\n",
    "    - If the rows cannot be evenly divided, some buckets may contain one more row than others.\n",
    "\n",
    "**Use Cases for ntile():**\n",
    "- **Percentile Calculations**: Dividing data into quantiles like quartiles, deciles, etc., for analysis such as statistical summaries.\n",
    "- **Categorization**: Assigning categories to data points based on ranks, such as splitting data into high, medium, and low categories.\n",
    "- **Segmenting Data**: Segmenting users, customers, or employees based on certain metrics (e.g., income, sales performance, etc.) into equal-sized buckets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "799628c4-9887-49bd-a9a3-002423190f2b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Example**: Each employee is assigned a Salary_Quartile based on their salary relative to others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "03d86349-490f-4ac8-b88d-416fff409d3b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define the Window specification (ordering by Salary in descending order)\n",
    "window_spec_ex = Window.orderBy(col(\"Salary\").desc())\n",
    "df.withColumn(\"Salary_Quartile\",ntile(4).over(window_spec_ex)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bb63a47e-20a7-421d-9d75-3fd71b63ad30",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "window_spec = Window.partitionBy(\"Category\").orderBy(col(\"TotalSales\").desc())\n",
    "tot_salesdf.withColumn(\"ntile\",ntile(2).over(window_spec)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5248d9ec-c9d9-4b11-b864-a4c57e8a7e04",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Cumulative Distribution Window Function\n",
    "\n",
    "The ```cume_dist()``` window function in PySpark calculates the cumulative distribution of a value within a partition. It provides a measure of how the current row compares to all the other rows in the partition based on a specific ordering. It calculates the relative rank of each row in terms of its value in the specified partition, normalized between 0 and 1.\n",
    "\n",
    "i.e. cume_dist() gives the fraction of rows in the partition that have a value less than or equal to the current row's value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "323cfa06-4276-43c1-9c79-c248c090eef7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Apply cume_dist function to calculate the cumulative distribution of salaries\n",
    "window_spec_ex  = Window.partitionBy(\"department\").orderBy(\"salary\")\n",
    "df.withColumn(\"CumeDist\", cume_dist().over(window_spec_ex)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cb4f59a9-04ee-4880-b60e-dc81b87409ce",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "window_spec = Window.partitionBy(\"Category\").orderBy(col(\"TotalSales\"))\n",
    "tot_salesdf.withColumn(\"CumeDist\",cume_dist().over(window_spec)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "05c4fa07-7703-41a4-834c-a27917a32703",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Lag Window Function\n",
    "\n",
    "The ```lag()``` window function in PySpark is used to access data from a previous row in the same result set, without needing to perform a self-join. This function provides a way to compare the current row's value to previous row values based on a specified order within a partition.\n",
    "\n",
    "**Syntax:** ```df.withColumn(\"lag_column\", F.lag(column_name, offset, default_value).over(window_spec))```\n",
    "\n",
    "**Where:**\n",
    "- **column_name**: The column from which you want to retrieve the lagged value.\n",
    "- **offset**: (Optional) The number of rows before the current row to access. By default, it's 1.\n",
    "- **default_value**: (Optional) The value to return if there is no previous row. By default, it's None.\n",
    "- **window_spec**: Defines how to partition and order the data.\n",
    "\n",
    "**How it Works:**\n",
    "- The lag() function returns the value of the column from a previous row based on the specified offset (number of rows before the current row).\n",
    "- It requires a window specification that defines how the data should be partitioned and ordered.\n",
    "- If there is no previous row (e.g., for the first row), the lag() function returns the default value or null if no default value is provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0a14cd6b-2440-4dc6-8d6b-400a476e9166",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "window_spec_ex  = Window.partitionBy(\"department\").orderBy(\"salary\")\n",
    "df.withColumn(\"lag\",lag(\"salary\").over(window_spec_ex)) \\\n",
    "    .withColumn(\"lead\",lead(\"salary\").over(window_spec_ex)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "323b661d-4555-4228-a6a8-e9f7934aae01",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Filter for the year 2023, Group by Month and calculate total sales\n",
    "df_monthly_sales = (salesdf\n",
    "                    .filter(year(col(\"Date\")) == 2023)\n",
    "                    .groupBy(month(col(\"Date\")).alias(\"Month\"))\n",
    "                    .agg(\n",
    "                        round(sum(\"Sales\"),2).alias(\"TotalSales\")\n",
    "                    ).sort('Month'))\n",
    "\n",
    "df_monthly_sales.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ce34278a-3d12-4ee8-9fe3-efec514fd73e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define the Window specification (order by Month)\n",
    "window_spec = Window.orderBy(\"Month\")\n",
    "\n",
    "# Apply lag() and lead() to get the previous and next month's sales\n",
    "df_with_lag_lead = df_monthly_sales.withColumn(\n",
    "    \"PreviousMonthSales\", \n",
    "    lag(\"TotalSales\", 1).over(window_spec)\n",
    ").withColumn(\n",
    "    \"NextMonthSales\", \n",
    "    lead(\"TotalSales\", 1).over(window_spec)\n",
    ")\n",
    "\n",
    "df_with_lag_lead.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "697be22b-b20b-4de5-896f-f58d035c1922",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Find top order place by each customer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "da320cd2-0113-4db0-a406-d5ff444ef20b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "customers = [\n",
    "    (1, \"Robert\", \"NY\"),\n",
    "    (2, \"Denial\", \"CL\"),\n",
    "    (3, \"Demitri\", \"LA\"),\n",
    "    (4, \"Rabita\", \"LA\")\n",
    "]\n",
    "\n",
    "cdf = spark.createDataFrame(customers, schema=(\"cust_id\", \"name\", \"city\"))\n",
    "cdf.show()\n",
    "orders = [\n",
    "    (1, \"Tablet\", 670),\n",
    "    (1, \"Head Phone\", 250),\n",
    "    (2, \"Mouse\", 199),\n",
    "    (2, \"Mobile\", 485),\n",
    "    (4, \"Tablet X\", 942),\n",
    "    (3, \"Head Phone X+\", 399),\n",
    "    (1, \"Mobile\", 289),\n",
    "    (3, \"SSD\", 459),\n",
    "    (4, \"Monitor\", 549)\n",
    "]\n",
    "odf = spark.createDataFrame(orders, schema=('cust_id', 'product', 'sales'))\n",
    "odf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8dbda3a2-eb26-4220-8222-f50b60f56bdd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "joined_df = cdf.join(odf, on=\"cust_id\")\n",
    "joined_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "31b30994-32a5-4396-9787-41f6476dfab1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "window_spec = Window.partitionBy(\"cust_id\").orderBy(desc(\"sales\"))\n",
    "ranked_df = (joined_df\n",
    "             .withColumn(\"rank\", rank().over(window_spec))\n",
    "             .filter(col(\"rank\") == 1)\n",
    "             .select(\"cust_id\", \"name\", \"city\", \"product\", \"sales\")\n",
    "             .sort(desc(\"sales\")))\n",
    "\n",
    "ranked_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a466a948-fb5a-4218-91e4-a7f36f159f8b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# spark.stop()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "WindowFunction",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

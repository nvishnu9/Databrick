{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4138fd70-e00b-4d56-9d49-28d885c6829b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "import uuid\n",
    "import json\n",
    "import random\n",
    "from datetime import datetime, timedelta\n",
    "import pandas as pd\n",
    "\n",
    "# Table names\n",
    "bronze_table = \"bronze_sales_tbl\"\n",
    "silver_table = \"silver_sales_tbl\"\n",
    "gold_table   = \"gold_sales_tbl\"\n",
    "\n",
    "# Paths for Auto Loader input and checkpoints\n",
    "sales_path = \"/sales_order\"\n",
    "bronze_checkpoint = \"/checkpoint/bronze\"\n",
    "silver_checkpoint = \"/checkpoint/silver\"\n",
    "gold_checkpoint   = \"/checkpoint/gold\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c234e823-6c45-471e-a69a-22d2677434d7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(\"USE CATALOG hive_metastore\")\n",
    "spark.sql(\"DROP DATABASE IF EXISTS medallion CASCADE\")\n",
    "spark.sql(\"CREATE DATABASE medallion\")\n",
    "spark.sql(\"USE medallion\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9a439517-4d0e-42d9-b555-3362d566b97e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(\"SHOW TABLES\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9ea589a5-e5eb-43d1-9381-3d5011e5267e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Generate static customer table\n",
    "customer_data = [\n",
    "    (1, \"Alice\", \"North\"),\n",
    "    (2, \"Bob\", \"South\"),\n",
    "    (3, \"Charlie\", \"East\"),\n",
    "    (4, \"Diana\", \"West\"),\n",
    "    (5, \"Eve\", \"North\"),\n",
    "    (6, \"Frank\", \"South\"),\n",
    "    (7, \"Grace\", \"East\"),\n",
    "    (8, \"Hank\", \"West\"),\n",
    "    (9, \"Ivy\", \"North\"),\n",
    "    (10, \"Jack\", \"South\"),\n",
    "    (11, \"Kathy\", \"East\"),\n",
    "    (12, \"Leo\", \"West\"),\n",
    "    (13, \"Mona\", \"North\"),\n",
    "    (14, \"Nina\", \"South\"),\n",
    "    (15, \"Oscar\", \"East\")\n",
    "]\n",
    "customer_df = spark.createDataFrame(customer_data, [\"customer_id\", \"customer_name\", \"region\"])\n",
    "customer_df.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"customers\")\n",
    "\n",
    "# Generate static product table\n",
    "product_data = [\n",
    "    (101, \"Laptop\", 800),\n",
    "    (102, \"Mouse\", 25),\n",
    "    (103, \"Keyboard\", 45),\n",
    "    (104, \"Monitor\", 180),\n",
    "    (105, \"Headphones\", 60),\n",
    "    (106, \"Webcam\", 70),\n",
    "    (107, \"Printer\", 150),\n",
    "    (108, \"Tablet\", 300),\n",
    "    (109, \"Smartphone\", 600),\n",
    "    (110, \"Speakers\", 120),\n",
    "    (111, \"Router\", 90),\n",
    "    (112, \"External Hard Drive\", 100)\n",
    "]\n",
    "product_df = spark.createDataFrame(product_data, [\"product_id\", \"product_name\", \"unit_price\"])\n",
    "product_df.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"products\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b011cb77-f55e-45fa-b31f-9a23855a3de2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Sales Data Generator Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8c2204ba-18bc-43b9-8828-12e0cbdf4652",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Ensure the directory exists\n",
    "dbutils.fs.mkdirs(sales_path)\n",
    "\n",
    "def generate_sales_data(num_records=5):\n",
    "    sales = []\n",
    "    for _ in range(num_records):\n",
    "        sales.append({\n",
    "            \"order_id\": str(uuid.uuid4()),\n",
    "            \"customer_id\": random.choice([1, 2, 3, 4, 5]),\n",
    "            \"product_id\": random.choice([101, 102, 103, 104, 105]),\n",
    "            \"quantity\": random.randint(1, 5),\n",
    "            \"order_timestamp\": (datetime.now() - timedelta(minutes=random.randint(1, 60))).strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        })\n",
    "\n",
    "    file_name = f\"/dbfs{sales_path}/sales_{uuid.uuid4()}.json\"\n",
    "    \n",
    "    with open(file_name, \"w\") as f:\n",
    "        for record in sales:\n",
    "            f.write(json.dumps(record) + \"\\n\")  # Write as newline-delimited JSON (NDJSON)\n",
    "\n",
    "    print(f\"{num_records} sales records written to {file_name}\")\n",
    "\n",
    "def listStream():\n",
    "    for q in spark.streams.active:\n",
    "      print(\"Id: \", q.id, \"Streaming: \", q.isActive)\n",
    "\n",
    "def stopStream():\n",
    "    for q in spark.streams.active:\n",
    "      q.stop()\n",
    "      q.awaitTermination()\n",
    "\n",
    "def cleanup():\n",
    "    stopStream()\n",
    "    dbutils.fs.rm(\"/checkpoint\", True)\n",
    "    dbutils.fs.rm(\"/schema\", True)\n",
    "    spark.sql(\"DROP DATABASE IF EXISTS medallion CASCADE\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Sales-Data-Generator",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

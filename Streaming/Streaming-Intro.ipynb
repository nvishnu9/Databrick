{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3478ccdd-65cb-458c-be68-f35b6eea3035",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Streaming Data with Apache Spark\n",
    "\n",
    "Apache Spark offers comprehensive capabilities for processing streaming data, allowing you to carry out real-time analytics effectively. Central to this capability is the concept of a data stream, which forms the core unit of processing. Before working with streaming data in Spark, it’s important to first understand what a data stream is and what makes it unique."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f151e320-7aef-4118-a4a2-f4786970055a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### What Is a Data Stream?\n",
    "A data stream is an unbounded sequence of data that continuously flows from different sources such as sensors, log files, or social media feeds. As fresh data is produced, it gets appended to the stream, creating a dynamic and ever-evolving dataset. Some examples of data streams include:\n",
    "\n",
    "- **Social media feeds**\n",
    "  A continuous flow of posts containing text, user details, and timestamps, which can be processed to analyze trends, sentiments, or user behavior.\n",
    "\n",
    "- **Sensor readings**\n",
    "  Data such as temperature and humidity measurements from a network of sensors in a smart building, used to optimize energy usage.\n",
    "\n",
    "- **Log data**\n",
    "  Streams of log messages generated by servers that capture system events and error information to help monitor performance or detect security issues.\n",
    "\n",
    "Processing these data streams introduces unique challenges because of their constantly growing and changing nature. To manage continuous flows of data, there are generally two main strategies:\n",
    "\n",
    "- **Recompute**\n",
    "  This traditional method reprocesses the entire dataset whenever new data arrives to ensure accuracy. Although reliable, it can become resource-intensive and slow when dealing with large volumes of data.\n",
    "\n",
    "- **Incremental processing**\n",
    "  This approach creates custom logic to track and process only the new data added since the previous update. By focusing solely on recent changes, incremental processing significantly reduces computational overhead and improves efficiency.\n",
    "\n",
    "A key tool that enables incremental processing in Apache Spark is Spark Structured Streaming, which streamlines the development of scalable, real-time data pipelines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "784a3ba4-eb4e-420a-912e-df8496700c1e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Spark Structured Streaming Overview\n",
    "\n",
    "* A scalable stream processing engine in Apache Spark.\n",
    "* Transforms how data streams are processed and queried.\n",
    "* Automatically detects new data as it arrives.\n",
    "* Incrementally persists results to target sinks (e.g., durable storage like files or tables).\n",
    "\n",
    "**Core Concept**\n",
    "\n",
    "* Treats live data streams as unbounded, continuously growing tables.\n",
    "* Each new record is appended as a new row.\n",
    "* Enables the use of familiar SQL and DataFrame operations on streaming data.\n",
    "* Unifies batch and streaming processing, no need for separate stacks.\n",
    "* Simplifies migration from batch Spark jobs to streaming jobs.\n",
    "\n",
    "**Append-Only Requirement**\n",
    "\n",
    "* Streaming sources must be *append-only*.\n",
    "* Data can only be added—no updates, deletions, or overwrites.\n",
    "* If a source allows changes to existing data, it is not suitable for Structured Streaming.\n",
    "* Ensuring compliance with this requirement is essential for streaming workflows.\n",
    "\n",
    "**Delta Lake Integration**\n",
    "\n",
    "* Spark Structured Streaming supports integration with:\n",
    "\n",
    "  * File directories\n",
    "  * Messaging systems like Kafka\n",
    "  * Delta Lake tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e0ec559a-792f-4ae3-9c8b-794efbb4cbb8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### DataStreamReader in PySpark\n",
    "\n",
    "* Use `spark.readStream` to read a Delta Lake table as a streaming source.\n",
    "* Enables processing of both existing and new data.\n",
    "* Returns a *streaming DataFrame* for transformations.\n",
    "\n",
    "  `streamDF = spark.readStream.table(\"source_table\")`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "980e927a-850b-4b59-a250-e6c57d9124f5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### DataStreamWriter in PySpark\n",
    "\n",
    "* After transformations, persist results with `writeStream`.\n",
    "* Allows configuration of output options and durable storage targets.\n",
    "\n",
    "  `streamDF.writeStream.table(\"target_table\")`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2fbc3429-08b7-406b-a18c-6e2d436d7a11",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Read streaming data from a Delta table\n",
    "# streamDF = spark.readStream.table(\"source_table\")\n",
    "\n",
    "# Write the streaming data to another Delta table\n",
    "# streamDF.writeStream \\\n",
    "#     .trigger(processingTime=\"10 seconds\")\n",
    "#     .outputMode(\"append\") \\\n",
    "#     .option(\"checkpointLocation\", \"/path/to/checkpoint\") \\\n",
    "#     .table(\"target_table\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d14a3e8b-499e-444b-8f79-e86687b840b4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Streaming Query Configurations\n",
    "When configuring `DataStreamWriter`, several important settings control how streaming queries behave:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cce8205b-9295-4030-80fa-87f5966cd084",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Trigger Intervals\n",
    "\n",
    "* The `trigger` method determines **how frequently** the system processes new data.\n",
    "* This timing mechanism is called the **trigger interval**.\n",
    "* There are two main trigger modes:\n",
    "\n",
    "  * **Continuous Trigger**\n",
    "\n",
    "    * Processes data *continuously* as soon as it arrives.\n",
    "    * Suitable for low-latency use cases.\n",
    "\n",
    "  * **Triggered (Batch) Trigger**\n",
    "\n",
    "    * Processes data at **fixed time**.\n",
    "    * Helps balance resource usage and latency.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4eee0823-a541-4c35-a8d9-052b8385441d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "| **Mode**                          | **Usage**                              | **Behavior**                                                                                                                                                                                          |\n",
    "| --------------------------------- | -------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n",
    "| **Continuous**                    | `.trigger(processingTime=\"5 minutes\")` | - Processes data **continuously in micro-batches** at regular intervals.<br>- Default interval: **500 ms** (`processingTime=\"500ms\"`).<br>- Enables near-real-time processing.                        |\n",
    "| **Triggered Once** *(deprecated)* | `.trigger(once=True)`                  | - Processes **all available data in a single micro-batch**, then stops automatically.<br>- May cause out-of-memory errors if data volume is large.<br>- Deprecated since Databricks Runtime 11.3 LTS. |\n",
    "| **Triggered AvailableNow**        | `.trigger(availableNow=True)`          | - Processes **all available data in multiple micro-batches** until done, then stops.<br>- More scalable for large datasets.<br>- Ensures efficient resource use.                                      |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ee489652-cc2c-40f6-9a30-97f302cf6a71",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Output Modes\n",
    "\n",
    "**Append Mode:** `.outputMode(\"append\")`\n",
    "\n",
    "- Default output mode. \n",
    "- Each trigger writes only new incoming rows since the last checkpoint.\n",
    "- Suitable when you need a growing dataset.\n",
    "\n",
    "  \n",
    "\n",
    "**Complete Mode:** `.outputMode(\"complete\")`\n",
    "\n",
    "- Recomputes all results each time.\n",
    "- Overwrites the entire target (e.g., updating aggregates).\n",
    "- Useful for maintaining up-to-date summary tables.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "830b5834-7c19-4abb-b51c-70d3831414b1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Checkpointing\n",
    "\n",
    "`.option(\"checkpointLocation\", \"/path/to/checkpoint\")`\n",
    "\n",
    "**Purpose:**\n",
    "\n",
    "- Stores progress and metadata about the streaming query.\n",
    "- Ensures recovery after failures—processing resumes from the last checkpoint, not from scratch.\n",
    "\n",
    "**Key Points:**\n",
    "\n",
    "- Checkpoints are saved to reliable storage (e.g., DBFS, Amazon S3, Azure Storage).\n",
    "- Cannot be shared across multiple streaming queries.\n",
    "- Every streaming write operation requires its own checkpoint location to maintain separate state and guarantees."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5d1b3360-6420-4abe-bac5-6eab01d1604b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Structured Streaming Guarantees\n",
    "Spark Structured Streaming provides two main guarantees to ensure reliable, fault-tolerant processing:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "afa39410-1983-4b22-aad4-a65ff27765f3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Fault Recovery\n",
    "\n",
    "* If failures occur (e.g., node crashes or network issues), processing can **resume from the last successful point**.\n",
    "* This recovery relies on:\n",
    "\n",
    "  * **Checkpointing** - saves the progress and state of the stream.\n",
    "  * **Write-Ahead Logs (WALs)** - capture the **offset range** for each trigger, allowing replay of unprocessed data.\n",
    "\n",
    "* **Repeatable Data Sources** are critical:\n",
    "\n",
    "  * Sources like cloud object storage and pub/sub messaging systems (Kafka, Event Hubs) allow the same data to be read repeatedly.\n",
    "  * This ensures data can be **safely reprocessed after a failure** without loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5de60723-8676-45d8-953b-79b136546152",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Exactly-Once Semantics\n",
    "\n",
    "* Every record is processed **exactly one time**, even if failures and retries occur.\n",
    "* This is possible through **idempotent sinks**, which:\n",
    "\n",
    "  * Allow multiple writes for the same records without creating duplicates.\n",
    "  * Use the **offsets as unique identifiers** to detect and ignore duplicate writes.\n",
    "* **Key Benefits:**\n",
    "\n",
    "  * No data loss.\n",
    "  * No duplicate entries.\n",
    "  * Guaranteed consistent output in the sink."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "68e47f09-9a23-4ac2-91e6-26af3eb84c37",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Unsupported Operations\n",
    "\n",
    "* Since streaming data is **infinite/unbounded**, some operations common in batch processing are **not supported** or are limited in streaming:\n",
    "\n",
    "  * **Sorting** the entire dataset (cannot fully sort infinite data).\n",
    "  * **Global deduplication** across all time.\n",
    "* **Alternatives:**\n",
    "\n",
    "  * Use **windowing** (e.g., tumbling, sliding windows) to group data into bounded chunks for aggregation or deduplication.\n",
    "  * Use **watermarking** to manage late-arriving data.\n",
    "* **Note:**\n",
    "\n",
    "  * Deep knowledge of these techniques is generally required for **Databricks Data Engineer Professional certification**, not for Associate-level."
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Streaming-Intro",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
